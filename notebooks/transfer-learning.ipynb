{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../dataset/train'\n",
    "TEST_DIR = '../dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_CHARACTERS = {}\n",
    "\n",
    "for idx, training_class in enumerate(os.listdir(TRAIN_DIR)):\n",
    "    MAP_CHARACTERS[idx] = training_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14860 images belonging to 32 classes.\n",
      "Found 1636 images belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255., validation_split=.1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=TRAIN_DIR, class_mode='categorical', target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=TRAIN_DIR, class_mode='categorical', target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4142 images belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR, class_mode='categorical', target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet')\n",
    "resnet.trainable = False\n",
    "\n",
    "# tf.keras.applications.EfficientNetB5(\n",
    "# include_top=True, weights='imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.Dense(32, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50v2 (Functional)      (None, None, None, 2048)  23564800  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 23,698,016\n",
      "Trainable params: 133,216\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "465/465 [==============================] - 455s 975ms/step - loss: 2.3666 - accuracy: 0.3615 - val_loss: 1.6992 - val_accuracy: 0.5465\n",
      "Epoch 2/10\n",
      "465/465 [==============================] - 449s 966ms/step - loss: 1.1672 - accuracy: 0.6943 - val_loss: 1.5322 - val_accuracy: 0.5880\n",
      "Epoch 3/10\n",
      "465/465 [==============================] - 438s 942ms/step - loss: 0.8803 - accuracy: 0.7641 - val_loss: 1.4035 - val_accuracy: 0.6229\n",
      "Epoch 4/10\n",
      "465/465 [==============================] - 436s 937ms/step - loss: 0.6815 - accuracy: 0.8166 - val_loss: 1.3925 - val_accuracy: 0.6351\n",
      "Epoch 5/10\n",
      "465/465 [==============================] - 454s 976ms/step - loss: 0.5617 - accuracy: 0.8506 - val_loss: 1.4414 - val_accuracy: 0.6363\n",
      "Epoch 6/10\n",
      "465/465 [==============================] - 453s 975ms/step - loss: 0.4621 - accuracy: 0.8750 - val_loss: 1.4987 - val_accuracy: 0.6302\n",
      "Epoch 7/10\n",
      "465/465 [==============================] - 436s 938ms/step - loss: 0.3810 - accuracy: 0.9003 - val_loss: 1.5242 - val_accuracy: 0.6357\n",
      "Epoch 8/10\n",
      "465/465 [==============================] - 385s 828ms/step - loss: 0.3199 - accuracy: 0.9226 - val_loss: 1.6097 - val_accuracy: 0.6461\n",
      "Epoch 9/10\n",
      "465/465 [==============================] - 384s 826ms/step - loss: 0.2674 - accuracy: 0.9316 - val_loss: 1.6920 - val_accuracy: 0.6326\n",
      "Epoch 10/10\n",
      "465/465 [==============================] - 385s 828ms/step - loss: 0.2284 - accuracy: 0.9429 - val_loss: 1.7878 - val_accuracy: 0.6186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f955bf2f1f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
